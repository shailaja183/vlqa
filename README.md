# VLQA
Website, Code and Data for Visuo-Linguistic Question Answering (VLQA), Findings of EMNLP 2020

Visuo-Linguistic Question Answering (VLQA) is a dataset for joint reasoning over visuo-linguistic context.

# Website/: 
https://shailaja183.github.io/vlqa/

# Download Dataset: 
Train Set/Val Set/Test Set and Images

# Starter Code: 
Code for multimodal baselines HOLE, LXMERT, ViLBERT, VLBERT, VisualBERT and DQA-Net is available at
https://github.com/shailaja183/vlqa/baselines

# Paper and Supplementary Material:
https://arxiv.org/pdf/2005.00330.pdf

# Citation
If you find our dataset or model helpful, please cite our paper :-)<br/>
@misc{sampat2020diverse,<br/>
title={Visuo-Linguistic Question Answering (VLQA) Challenge},<br/>
author={Shailaja Sampat, Yezhou Yang and Chitta Baral},<br/>
year={2020},<br/>
eprint={2005.00330},<br/>
archivePrefix={arXiv},<br/>
primaryClass={cs.CV}<br/>
}
